# config.py
LLM_MODEL = "llama2"  # Make sure you've run: ollama pull llama2
TEMPERATURE = 0.2
MAX_TOKENS = 2048  # Adjust based on your local setup